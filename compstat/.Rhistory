lines(x,predict(fit,x),col="red")
estimates[j,i] = predict(fit,xtest)
}
abline(v = xtest)
}
test_errors = rep(0,6)
for(i in 1:6){
for(j in 1:1000){
y = f(x)+rnorm(n,0,sigma)
ytest = f(xtest)+rnorm(1,0,sigma)
fit = loess(y~x, span = span[i]) #perform local regression
test_errors[i] = test_errors[i]+(predict(fit,xtest)-ytest)^2
}
}
test_errors = test_errors/1000 ##looks like alpha = 0.3 lower test error at this point
# 2.79 h??r f??r alpha = 0.1
test_errors
set.seed(0)
n<-100
z1<-rnorm(n)
z2<-rnorm(n)
M=matrix(c(1,1,0.1,-0.1),2,2)
X=t(M%*%rbind(z1,z2))
beta<-c(0.5,-1.0)
x1=X[,1]
x2=X[,2]
y=5+beta[1]*x1+beta[2]*x2 +rnorm(n)
X = cbind(1,X) # glöm ej lägga till kolonn för intercept!
plot(x1,x2,type="p",col="red",main="training data") ##highly correlated...
fit = lm(y~x1+x2) #can't say any are significant individually because of correlated but ANOVA says atleast some not 0
## can see they have high standard error also because of correlated
##what is p-value for bhat_1?
XtX.inv = solve(t(X)%*%X)
bhat = XtX.inv%*%t(X)%*%y
RSS = sum((y-X%*%bhat)^2)
sigma = sqrt(1/97*RSS)
se = sqrt(sigma^2*XtX.inv) ##detta matris
t_val = bhat[2]/se[2,2]
p_val = 2*(1-pt(t_val,df=97)) # so don't reject!
# the ANOVA F-test gives 0.003 so reject!
fit2 = lm(y ~ 1)
anova(fit,fit2)
r2 = 1-RSS/sum((y-mean(y))^2) ##TSS = sum (y_i-ybar)^2
fit3 = lm(y~x1) #so now it is significant!
summary(fit)
?solve
## simple linear regression
set.seed(21) # initializes the random number generator
x <- rnorm(40, 20, 3) # generates x-values
y <- 1 + 2 * x + 5 * rnorm(length(x)) # y-values = linear function(x) + error
reg <- lm(y ~ x) # fit of the linear regression
summary(reg) # output of selected results
plot(x, y) # scatter plot
abline(reg) # draw regression line
plot(reg, which = 1)
set.seed(21) # initializes the random number generator
x <- rnorm(40, 20, 3) # generates x-values
X = cbind(1,x) # need 1 first column design matrix!
par(mfrow = c(3,2))
slopes = numeric(100)
for(i in 1:100){
y <- 1 + 2*x + rnorm(length(x),0,5) # y-values = linear function(x) + error
fit = lm(y~x)
if(i <=3){
plot(x,y,col="red",type="p")
abline(fit)
plot(fit, which = 1)
}
slopes[i] = coef(fit)[2]
}
mean = mean(slopes)
sd = sqrt(var(slopes))
hist(slopes,prob=TRUE,col="red",breaks=10) # ??r den normal med det vi tror?
true_sd = 5*sqrt(solve(t(X)%*%X)[2,2])
vals = seq(1.4,2.6,length=100)
lines(vals,dnorm(vals,2,true_sd)) ##seems good!
rank(D)
D = c(4,8,3,-4,-1,2,5)
iter = 100000
D.sign.rank = rank(abs(D))*sign(D)
W = sum(D.sign.rank[D.sign.rank >= 0])
Ws = numeric(iter)
for(i in 1:iter){
multi = sample(c(-1,1),length(D),replace=TRUE)
new_D = D*multi
sign.rank = rank(abs(new_D))*sign(new_D)
Ws[i] = sum(sign.rank[sign.rank >= 0])
}
p_val = (sum(Ws >= W)+1)/(100001) #don't forget this one!
abs(p_val-wilcox.test(D,alternative="greater")$p.value)
rank(D)
rank(abs(D))
rank(abs(D))*sign(D)
sign.rank = rank(abs(D))*sign(D)
W = sum(sign.rank[sign.rank >= 0])
multi = sample(c(-1,1), length(D), replace=TRUE)
D = D*multi
sign.rank = rank(abs(D))*sign(D)
W = sum(sign.rank[sign.rank >= 0])
W
?wilcox.test
p_val = wilcox.test(D, alternative = "greater")$p.value
p_val
D = c(4,8,3,-4,-1,2,5)
iter = 100000
D.sign.rank = rank(abs(D))*sign(D)
W = sum(D.sign.rank[D.sign.rank >= 0])
Ws = numeric(iter)
for(i in 1:iter){
multi = sample(c(-1,1),length(D),replace=TRUE)
new_D = D*multi
sign.rank = rank(abs(new_D))*sign(new_D)
Ws[i] = sum(sign.rank[sign.rank >= 0])
}
p_val = (sum(Ws >= W)+1)/(100001) #don't forget this one!
abs(p_val-wilcox.test(D,alternative="greater")$p.value)
p_val
require(MASS)
data(immer)
set.seed(852)
Y1 = immer$Y1
Y2 = immer$Y2
D = Y1-Y2
D = D[ D != 0] #remove ties!
plot(Y1,Y2)
abline(a=0,b=1)
M = 1000000
#H0 : D symmetric around 0,
#HA : D symmetric around a, a > 0
D.sign.rank = rank(abs(D))*sign(D)
W = sum(D.sign.rank[D.sign.rank >= 0])
Ws = numeric(M)
for(i in 1:M){
signs = sample(c(-1,1),length(D),replace=TRUE)
new_D = D*signs
sign.rank = rank(abs(new_D))*sign(new_D)
Ws[i] = sum(sign.rank[sign.rank >= 0])
}
hist(Ws ,prob="TRUE",col="blue")
abline(v = W, lwd="2")
p_val = (sum(Ws >= W)+1)/(M+1) #0.0027
p_val_wilcox = wilcox.test(D,alternative="greater")$p.value
x = rnorm(1000,0,1)
y = x/2+rnorm(1000,5,5)
M = 100000
#H0: x and y independent
#HA : x and y dependent
cor = cor(x,y)
cors = numeric(M)
for(i in 1:M){
new_y = sample(y,length(y),replace=FALSE)
cors[i] = cor(x,new_y)
}
hist(cors,col="red",prob="TRUE")
abline(v=cor, lwd = 2)
p_val = (sum(abs(cors) >= cor)+1)/(M+1) #0.01 so reject!
?chisq.test
?table
x = rnorm(1000,0,1)
y = -x/2+rnorm(1000,5,5)
M = 100000
#H0: x and y independent
#HA : x and y dependent
cor = cor(x,y)
cors = numeric(M)
for(i in 1:M){
new_y = sample(y,length(y),replace=FALSE)
cors[i] = cor(x,new_y)
}
hist(cors,col="red",prob="TRUE")
abline(v=cor, lwd = 2)
p_val = (sum(abs(cors) >= cor)+1)/(M+1) #0.01 so reject!
p_val
cor
x = rnorm(1000,0,1)
y = x/2+rnorm(1000,5,5)
M = 100000
#H0: x and y independent
#HA : x and y dependent
cor = cor(x,y)
cors = numeric(M)
for(i in 1:M){
new_y = sample(y,length(y),replace=FALSE)
cors[i] = cor(x,new_y)
}
hist(cors,col="red",prob="TRUE")
abline(v=cor, lwd = 2)
p_val = (sum(abs(cors) >= cor)+1)/(M+1) #0.01 so reject!
# DO ANOVA EXERCICE AND THEN POST MODEL INFERENCE!
data = read.csv("data_ex3.csv")
x = data$x
y = data$y
fit = lm(y~I(x)+I(x^2)+I(x^3)) #don't to poly here... changes coefficients!
plot(x,fitted.values(fit),type="p",col="red")
lines(x,y,type="p")
fstatistic = summary(fit)$fstatistic # can calculate this. Under H0 F-distributed
p_val = 1-pf(fstatistic[1],fstatistic[2],fstatistic[3]) #0.017 so reject H0
n = 20
x = seq(-25,30,length=n)
b0 = 0
b1 = 0.5
b2 = -0.003
b3 = 0.0001
crit = qf(0.95,3,16) #Under H0 är statistikan F_(3,16) och förkasta om p-val <= 0.05
# 250 data sets type 1 error (0.05 sign level)
count = 0
for(i in 1:1000){ #what is type 1 error = P(reject | H0)
y = b0+15*(rgamma(n,2,1)-2)
fit2 = lm(y~I(x)+I(x^2)+I(x^3))
if(summary(fit2)$fstatistic[1] >= crit){
count = count+1
}
}
type1_error = count/1000
count = 0
for(i in 1:1000){ #what is power = P(reject | HA)
y = b0+b1*x+b2*x^2+b3*x^3+15*(rgamma(n,2,1)-2)
fit2 = lm(y~I(x)+I(x^2)+I(x^3))
if(summary(fit2)$fstatistic[1] >= crit){
count = count+1
}
}
power = count/1000
x1 = rnorm(50,0,10)
x2 = rnorm(50,0,10)+x1/3
M = 500
count = 0
for(j in 1:M){
y = 1+2*x1+1.6*x2 + runif(50,-2,2)+rnorm(50,0,10)
fit = lm(y~x1+x2)
bhat = coef(fit)[2]
B = 50
bhats = rep(0,B)
for(i in 1:B){
inds = sample(1:50,50,replace=TRUE)
new_x1 = x1[inds]
new_x2 = x2[inds]
new_y = y[inds]
fitBS = lm(new_y~new_x1+new_x2)
bhats[i] = coef(fitBS)[2]
}
ci = c(2*bhat-quantile(bhats,p=0.975),2*bhat-quantile(bhats,p=0.025))
if( ci[1] <= 2 & 2 <= ci[2]){
count = count+1
}
}
print(count/M) #not really good, want it to be 95% but ok!
x1 = rnorm(50,0,10)
x2 = rnorm(50,0,10)+x1/3
M = 500
count = 0
for(j in 1:M){
y = 1+2*x1+1.6*x2 + runif(50,-2,2)
fit = lm(y~x1+x2)
bhat = coef(fit)[2]
B = 100
bhats = rep(0,B)
for(i in 1:B){
inds = sample(1:50,50,replace=TRUE)
new_x1 = x1[inds]
new_x2 = x2[inds]
new_y = y[inds]
fitBS = lm(new_y~new_x1+new_x2)
bhats[i] = coef(fitBS)[2]
}
ci = c(2*bhat-quantile(bhats,p=0.975),2*bhat-quantile(bhats,p=0.025))
if( ci[1] <= 2 & 2 <= ci[2]){
count = count+1
}
}
print(count/M) #not really good, want it to be 95% but ok!
x1 = rnorm(50,0,10)
x2 = rnorm(50,0,10)+x1/3
M = 500
count = 0
for(j in 1:M){
y = 1+2*x1+1.6*x2 + runif(50,-2,2)
fit = lm(y~x1+x2)
bhat = coef(fit)[2]
B = 100
bhats = rep(0,B)
for(i in 1:B){
inds = sample(1:100,100,replace=TRUE)
new_x1 = x1[inds]
new_x2 = x2[inds]
new_y = y[inds]
fitBS = lm(new_y~new_x1+new_x2)
bhats[i] = coef(fitBS)[2]
}
ci = c(2*bhat-quantile(bhats,p=0.975),2*bhat-quantile(bhats,p=0.025))
if( ci[1] <= 2 & 2 <= ci[2]){
count = count+1
}
}
print(count/M) #not really good, want it to be 95% but ok!
?median
library(MASS)
?fitdistr
library(boot)
?boot.ci
?fitdistr
?bs
x = seq(-1, 1, length = 101)
y  = x+4*cos(7*x)+rnorm(101,0, 1)
par(mfrow = c(1,3))
knots = c(quantile(x,0.25), quantile(x, 0.5), quantile(x, 0.75))
fit1 = lm(y~ bs(x, knots = knots, degree = 3)) # cubic spline 3 knots. bs gives x,x^2, (x-eps_1)_+^3,(x-eps_2)_+^3,(x-eps_3)_+^3...... s ger smooth spline
plot(x, y, type="p", main = "cubic spline 3 knots uniformly")
lines(x, fitted.values(fit1), col="red")
fit2 = gam(y ~ s(x, df = 4))
plot(x, y, type="p", main = "smoothing spline",xlim = c(-2,2))
lines(seq(-2,2,length=100), predict(fit2, data.frame(x=seq(-2,2,length=100)) ), col="red")
fit3 = loess(y~ x, span = 0.5)
plot(x, y, type="p", main = "local regression")
lines(x, predict(fit3,x), col="red")
x = seq(-1,1, length = 400)
y = x+rnorm(400,0,1)
fit = lm(y ~ poly(x, degree=20))
par(mfrow = c(1,2))
plot(x, fitted.values(fit), type="p")
plot(x,y,type="p") # for large polynomial degree seems to be very flexible near boundary!
# Panini: Monte Carlo test
# Background:
#   There are 682 different Panini cards.
#   The cards are packaged in packs with 5 cards.
#   100 of such packs (so 500 cards) are packaged in a box.
#   Panini seems to say that cards are packaged at random.
#   My children and their friends suspect that boxes contain
#      "too few" duplicates: "I will buy a box to have many
#       different cards, and then some separate packs to swap."
# Results from a box with 100 packs:
#   6 duplicates in 50 packs (250 cards)
# How likely is it to see this result (or more extreme ones)
#   when the cards are indeed packaged at random with replacement?
# Formally:
# H0: cards are packaged at random with replacement
# We want to compute the p-value: the probability of observing our
#   results (or more extreme ones, i.e., even fewer duplicates)
#   when the null hypothesis is true.
# So we need to know the distribution of the number of
#   duplicates under H0. This is cumbersome to analyze analytically,
#   but easy to simulate!
nr.cards <- 682    # number of unique cards
nsimul <- 100000   # number of simulations
npack <- 50        # number of packs
# Function that simulates sampling cards at random with replacement.
# Returns the number of duplicates in npack packs.
simulate.duplicate <- function(){
res <- sample(1:nr.cards, 5*npack, replace=T)
return(5*npack - length(unique(res)))  # nr of duplicates
}
res.1 <- replicate(nsimul,simulate.duplicate())
summary(res.1)
hist(res.1, xlim=c(0,max(res.1)), prob=T)
abline(v=6, col="red")
(pval.1 <- (1+sum(res.1 <= 6))/(nsimul+1))
(rej.1 <- quantile(res.1, 0.05)-1)  # boundary of rejection region at alpha=0.05
res.1
quantile(res.1, 0.05)
a = c(1:5)
quantile(a, 0.05)
quantile(a,0.95)
quantile(a, 0.7)
a = c(1:10)
a
quantile(a,0.4)
quantile(a, 0.1)
quantile(a, 0.05
)
?cdf
a
ecdf(a)
plot(ecdf(a))
quantile(a,0.2)
quantile(a,0.0.15)
quantile(a,0.15
)
quantile(a,0.21)
quantile(a,0.2)
quantile(a,0.0.5)
quantile(a,0.05)
library(splines)
?splines
x = rnorm(100,0,1)
plot(ecdf(x))
?rnorm
l = seq(-2,3,length=100)
lines(lm,pnorm(l))
lines(l,pnorm(l))
x = rnorm(500,0,1)
plot(exdf(x))
plot(ecdf(x))
lines(l,pnorm(l))
?smooth.spline
# Based on ISLR Chapter 7 Lab: Non-linear Modeling
library(ISLR)
?Wage
attach(Wage)
# Polynomial Regression
# default implementation uses orthonogal polynomials:
fit=lm(wage~poly(age,4),data=Wage)
# to understand what is going on internally:
xx <- poly(age,4)  # constructs x-matrix
dim(xx)
par(mfrow=c(2,2))
for (j in 1:4){
plot(age, xx[,j])
}
round(cov(xx),5)  # columns are uncorrelated
coef(summary(fit))
round(coef(fit),2)
# estimated coefficients are the same in a model
# that goes up to a different degree:
round(coef(lm(wage~poly(age,2),data=Wage)),2)
# If you want, you can also use x, x^2, x^3 and x^4
# without orthoginalization:
fit2a=lm(wage~poly(age,4,raw=T),data=Wage)
round(coef(fit2a),2)
# or equivalently:
fit2b=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
round(coef(fit2b),2)
# But now estimated coefficients are not identical in
# models with different degrees:
fit3 = lm(wage~age+I(age^2), data=Wage)
round(coef(fit3),2)
# Why? See board.
# See Rcode12.
# Compute fitted values and confidence intervals
#   based on orthogonal polynomials
(agelims=range(age))
(age.grid=seq(from=agelims[1], to=agelims[2], by=1))
preds=predict(fit,newdata=list(age=age.grid),
se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,
preds$fit-2*preds$se.fit)
# plot results:
par(mfrow=c(1,1))
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial")
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# Fitted values and confidence intervals are the
# same when using non-orthogonal polynomials:
preds2a=predict(fit2a,newdata=list(age=age.grid),
se=TRUE)
max(abs(preds$fit-preds2a$fit))
max(abs(preds$se.fit-preds2a$se.fit))
# Test what degree we should use
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
round(coef(summary(fit.5),2),4)
#682 olika kort, varje paket innehåller 5 kort.
# H0 : packar likformigt med replacement
# HA : med replacement men 100 av korten 5 ggr mer sannolika än de andra, dvs packar för att få mer duplicates
# hur många paket ska vi göra testet med för att komma över 80% power?
# power = P(reject | HA)
set.seed(456)
M = 100000
npacks = seq(25,40,by = 5)
powers = rep(0, length(npacks))
duplicates_H0 <- function(npack){
sample = sample(1:682, npack*5, replace=TRUE)
5*npack-length(unique(sample))
}
duplicates_HA <- function(npack){
sample = sample(1:682, size = 5*npack, prob = c(rep(5/1082,100), rep(1/1082,582)), replace=TRUE)
5*npack-length(unique(sample))
}
for(i in 1:length(npacks)){
npack = npacks[i]
dupli_H0 = replicate(M, duplicates_H0(npack))
duplis_HA = replicate(M, duplicates_HA(npack))
crit = quantile(dupli_H0, 0.95)+1
power[i] = (sum(duplis_HA >= crit)+1)/(M+1)
}
# 30 packs is enough to do the test with so power is at least 80
# so now go buy 30 packets and if ## duplicates too large then reject H0. power of this test > 80%
power
library(hdi)
library(glmnet)
data(riboflavin)
x = riboflavin$x
y = riboflavin$y
grid = 10^seq(2,-2,length = 100)
lasso = glmnet(x,y,alpha = 1, lambda = grid) ## don't need intercept
coefs = coef(lasso)
test_lambda = coefs[,50]
folds = cut(1:71, breaks = 7, labels = FALSE)
errors = rep(0,2)
for(i in 1:7){
inds = which(folds == i)
xtest = x[inds,]
ytest = y[inds]
xtrain = x[-inds,]
ytrain = y[-inds]
lambda_min_lasso = cv.glmnet(xtrain,ytrain, alpha = 1)$lambda.min
lasso.mod = glmnet(xtrain,ytrain,lambda = lambda_min_lasso, alpha = 1)
pred_lasso = predict(lasso.mod,xtest)
errors[1] = errors[1] + sum((pred_lasso-ytest)^2)
lambda_min_ridge = cv.glmnet(xtrain,ytrain, alpha = 0)$lambda.min
ridge.mod = glmnet(xtrain,ytrain,lambda = lambda_min_ridge, alpha = 0)
pred_ridge = predict(ridge.mod, xtest)
errors[2] = errors[2] + sum((pred_ridge-ytest)^2)
}
errors = errors/71 ##0.260, and 0.325 so lasso procedure lower test errors
lambda_min = cv.glmnet(x,y, alpha = 1)$lambda.min
lasso.mod = coef(glmnet(x, y, lambda = lambda_min, alpha = 0))
errors
